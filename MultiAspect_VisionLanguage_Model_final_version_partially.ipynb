{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "#Preparing image and text embeddings for zero-shot and contrastive learning __V2\n",
        "# ==========================================\n",
        "# Author: Morvarid Rahbar\n",
        "# Student ID: 4033624008\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "beZMRGYkoLgk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGXvpQx3f7Q-",
        "outputId": "b80a2553-5acb-43bf-da1c-ac4ceec872bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report, hamming_loss, accuracy_score\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\" Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_6WbFSHf_n8",
        "outputId": "69fe8962-c5fd-497f-fb8d-f36cd70c5345"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/chexpert_data_v2/valid.csv\")\n",
        "\n",
        "image_embedding_path = \"/content/drive/MyDrive/Embedings/Image_embeding/image_embeddings_LRF30%.pt\"\n",
        "text_embedding_path = \"/content/drive/MyDrive/Embedings/Text_embeding/disease_text_embeddings.pt\"\n",
        "\n",
        "image_embeddings = torch.load(image_embedding_path)  # dict: key = img_path, value = [197, 64]\n",
        "text_embeddings = torch.load(text_embedding_path)    # dict: key = class name, value = [768]\n",
        "\n"
      ],
      "metadata": {
        "id": "nsG18E9igN6D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_columns = [\n",
        "    'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
        "    'Edema', 'Pleural Effusion', 'Pneumonia', 'Pneumothorax'\n",
        "]\n",
        "\n",
        "label_dict = {}\n",
        "prefix = \"CheXpert-v1.0-small/\"\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    path = row[\"Path\"]\n",
        "    if path.startswith(prefix):\n",
        "        path = path[len(prefix):]\n",
        "\n",
        "    label = []\n",
        "    for disease in disease_columns:\n",
        "        val = row[disease]\n",
        "        label.append(0.0 if pd.isna(val) else float(val))\n",
        "\n",
        "    label_dict[path] = torch.tensor(label)\n"
      ],
      "metadata": {
        "id": "PWnaZUwqgQr3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ÿ≥ÿßÿÆÿ™ ÿ™ŸÜÿ≥Ÿàÿ± ÿßŸÖÿ®ÿØ€åŸÜ⁄Ø‚ÄåŸáÿß Ÿà ŸÑ€åÿ®ŸÑ‚ÄåŸáÿß ---\n",
        "img_keys = list(image_embeddings.keys())\n",
        "img_tensor = torch.stack([image_embeddings[k] for k in img_keys])         # [N, 197, 64]\n",
        "img_tensor_pooled = img_tensor.max(dim=1).values  # [N, 64]\n",
        "\n",
        "labels_list = []\n",
        "valid_indices = []\n",
        "for idx, k in enumerate(img_keys):\n",
        "    label = label_dict[k]\n",
        "    if (label == -1).all():\n",
        "\n",
        "        continue\n",
        "    else:\n",
        "        label = torch.where(label == -1, torch.tensor(1.0), label)\n",
        "        labels_list.append(label)\n",
        "        valid_indices.append(idx)\n",
        "\n",
        "labels = torch.stack(labels_list)  # [M, 7]\n",
        "img_tensor_pooled = img_tensor_pooled[valid_indices]  # [M, 64]\n",
        "\n",
        "\n",
        "text_tensor = torch.stack([text_embeddings[d] for d in disease_columns])  # [7, 768]\n",
        "\n",
        "\n",
        "img_tensor_pooled = img_tensor_pooled.to(device)\n",
        "labels = labels.to(device)\n",
        "text_tensor = text_tensor.to(device)"
      ],
      "metadata": {
        "id": "N_4t8_UUgSnT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Project image and text embeddings to shared 256-dim space\n",
        "image_proj = nn.Linear(64, 256).to(device)\n",
        "text_proj = nn.Linear(768, 256).to(device)\n",
        "\n",
        "# Forward projection\n",
        "img_proj = F.normalize(image_proj(img_tensor_pooled), dim=1)     # [234, 256]\n",
        "txt_proj = F.normalize(text_proj(text_tensor), dim=1)     # [7, 256]\n",
        "\n",
        "# Similarity: [234, 7]\n",
        "similarity = img_proj @ txt_proj.T\n"
      ],
      "metadata": {
        "id": "ycDw4E1YgU6b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilities\n",
        "probs = torch.sigmoid(similarity)  # [234, 7]\n",
        "\n",
        "# Threshold = 0.5\n",
        "threshold = 0.8\n",
        "preds = (probs > threshold).float()\n",
        "\n",
        "# Accuracy (Exact Match)\n",
        "exact_match = (preds == labels).all(dim=1).float().mean()\n",
        "print(f\" Exact Match Accuracy: {exact_match.item():.4f}\")\n",
        "\n",
        "# Sample-wise Accuracy\n",
        "sample_accuracy = (preds == labels).float().mean()\n",
        "print(f\" Sample-wise Accuracy: {sample_accuracy.item():.4f}\")\n",
        "\n",
        "# Per-label Accuracy\n",
        "per_label_acc = (preds == labels).float().mean(dim=0)\n",
        "for i, disease in enumerate(disease_columns):\n",
        "    print(f\"{disease}: {per_label_acc[i].item():.4f}\")\n",
        "\n",
        "# Macro Accuracy\n",
        "macro_accuracy = per_label_acc.mean()\n",
        "print(f\" Macro (Mean Per-Label) Accuracy: {macro_accuracy.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpZFzQbqgWx-",
        "outputId": "f10f222d-a9d3-4fd1-d9b9-dcfacbf06962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Exact Match Accuracy: 0.3889\n",
            " Sample-wise Accuracy: 0.8114\n",
            "Atelectasis: 0.6581\n",
            "Cardiomegaly: 0.7094\n",
            "Consolidation: 0.8590\n",
            "Edema: 0.8077\n",
            "Pleural Effusion: 0.7137\n",
            "Pneumonia: 0.9658\n",
            "Pneumothorax: 0.9658\n",
            " Macro (Mean Per-Label) Accuracy: 0.8114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels.cpu().numpy()\n",
        "y_prob = probs.detach().cpu().numpy()\n",
        "\n",
        "# Threshold Tuning\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (y_prob > t).astype(int)\n",
        "    f1 = f1_score(y_true, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"üîß Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")\n",
        "\n",
        "# Final Predictions with best threshold\n",
        "y_pred = (y_prob > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UBMYniugZix",
        "outputId": "868a4d37-6c36-4d4f-977e-444e7a285c1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Best Threshold: 0.10 ‚Üí F1: 0.3010\n",
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.34      1.00      0.51        80\n",
            "    Cardiomegaly       0.29      1.00      0.45        68\n",
            "   Consolidation       0.14      1.00      0.25        33\n",
            "           Edema       0.19      1.00      0.32        45\n",
            "Pleural Effusion       0.29      1.00      0.45        67\n",
            "       Pneumonia       0.03      1.00      0.07         8\n",
            "    Pneumothorax       0.03      1.00      0.07         8\n",
            "\n",
            "       micro avg       0.19      1.00      0.32       309\n",
            "       macro avg       0.19      1.00      0.30       309\n",
            "    weighted avg       0.26      1.00      0.40       309\n",
            "     samples avg       0.19      0.61      0.27       309\n",
            "\n",
            " Hamming Loss: 0.8114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BCW learning"
      ],
      "metadata": {
        "id": "S5lQI6-Swtii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(list(image_proj.parameters()) + list(text_proj.parameters()), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "dataset_size = img_tensor_pooled.size(0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(dataset_size)\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        indices = perm[i:i+batch_size]\n",
        "\n",
        "        img_batch = img_tensor_pooled[indices]   # [B, 64]\n",
        "        label_batch = labels[indices]             # [B, 7]\n",
        "\n",
        "        img_proj_batch = image_proj(img_batch)   # [B, 256]\n",
        "        txt_proj = text_proj(text_tensor)        # [7, 256]\n",
        "\n",
        "        img_proj_norm = F.normalize(img_proj_batch, dim=1)\n",
        "        txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "        similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)  # [B, 7]\n",
        "\n",
        "        loss = criterion(similarity, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * img_batch.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / dataset_size\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuSlFOGwyDG",
        "outputId": "56277337-9346-4cf8-b9b6-b3e22ab43c4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 0.5909\n",
            "Epoch 2/100 - Loss: 0.5023\n",
            "Epoch 3/100 - Loss: 0.4983\n",
            "Epoch 4/100 - Loss: 0.4963\n",
            "Epoch 5/100 - Loss: 0.4946\n",
            "Epoch 6/100 - Loss: 0.4929\n",
            "Epoch 7/100 - Loss: 0.4913\n",
            "Epoch 8/100 - Loss: 0.4898\n",
            "Epoch 9/100 - Loss: 0.4884\n",
            "Epoch 10/100 - Loss: 0.4870\n",
            "Epoch 11/100 - Loss: 0.4858\n",
            "Epoch 12/100 - Loss: 0.4852\n",
            "Epoch 13/100 - Loss: 0.4841\n",
            "Epoch 14/100 - Loss: 0.4834\n",
            "Epoch 15/100 - Loss: 0.4829\n",
            "Epoch 16/100 - Loss: 0.4823\n",
            "Epoch 17/100 - Loss: 0.4817\n",
            "Epoch 18/100 - Loss: 0.4811\n",
            "Epoch 19/100 - Loss: 0.4807\n",
            "Epoch 20/100 - Loss: 0.4802\n",
            "Epoch 21/100 - Loss: 0.4797\n",
            "Epoch 22/100 - Loss: 0.4792\n",
            "Epoch 23/100 - Loss: 0.4787\n",
            "Epoch 24/100 - Loss: 0.4783\n",
            "Epoch 25/100 - Loss: 0.4778\n",
            "Epoch 26/100 - Loss: 0.4774\n",
            "Epoch 27/100 - Loss: 0.4769\n",
            "Epoch 28/100 - Loss: 0.4766\n",
            "Epoch 29/100 - Loss: 0.4760\n",
            "Epoch 30/100 - Loss: 0.4756\n",
            "Epoch 31/100 - Loss: 0.4752\n",
            "Epoch 32/100 - Loss: 0.4747\n",
            "Epoch 33/100 - Loss: 0.4743\n",
            "Epoch 34/100 - Loss: 0.4737\n",
            "Epoch 35/100 - Loss: 0.4734\n",
            "Epoch 36/100 - Loss: 0.4729\n",
            "Epoch 37/100 - Loss: 0.4726\n",
            "Epoch 38/100 - Loss: 0.4722\n",
            "Epoch 39/100 - Loss: 0.4719\n",
            "Epoch 40/100 - Loss: 0.4716\n",
            "Epoch 41/100 - Loss: 0.4714\n",
            "Epoch 42/100 - Loss: 0.4711\n",
            "Epoch 43/100 - Loss: 0.4707\n",
            "Epoch 44/100 - Loss: 0.4706\n",
            "Epoch 45/100 - Loss: 0.4703\n",
            "Epoch 46/100 - Loss: 0.4703\n",
            "Epoch 47/100 - Loss: 0.4699\n",
            "Epoch 48/100 - Loss: 0.4698\n",
            "Epoch 49/100 - Loss: 0.4696\n",
            "Epoch 50/100 - Loss: 0.4695\n",
            "Epoch 51/100 - Loss: 0.4694\n",
            "Epoch 52/100 - Loss: 0.4694\n",
            "Epoch 53/100 - Loss: 0.4696\n",
            "Epoch 54/100 - Loss: 0.4692\n",
            "Epoch 55/100 - Loss: 0.4692\n",
            "Epoch 56/100 - Loss: 0.4692\n",
            "Epoch 57/100 - Loss: 0.4690\n",
            "Epoch 58/100 - Loss: 0.4691\n",
            "Epoch 59/100 - Loss: 0.4686\n",
            "Epoch 60/100 - Loss: 0.4690\n",
            "Epoch 61/100 - Loss: 0.4687\n",
            "Epoch 62/100 - Loss: 0.4686\n",
            "Epoch 63/100 - Loss: 0.4685\n",
            "Epoch 64/100 - Loss: 0.4686\n",
            "Epoch 65/100 - Loss: 0.4684\n",
            "Epoch 66/100 - Loss: 0.4684\n",
            "Epoch 67/100 - Loss: 0.4682\n",
            "Epoch 68/100 - Loss: 0.4682\n",
            "Epoch 69/100 - Loss: 0.4680\n",
            "Epoch 70/100 - Loss: 0.4680\n",
            "Epoch 71/100 - Loss: 0.4680\n",
            "Epoch 72/100 - Loss: 0.4678\n",
            "Epoch 73/100 - Loss: 0.4680\n",
            "Epoch 74/100 - Loss: 0.4684\n",
            "Epoch 75/100 - Loss: 0.4679\n",
            "Epoch 76/100 - Loss: 0.4678\n",
            "Epoch 77/100 - Loss: 0.4678\n",
            "Epoch 78/100 - Loss: 0.4677\n",
            "Epoch 79/100 - Loss: 0.4677\n",
            "Epoch 80/100 - Loss: 0.4676\n",
            "Epoch 81/100 - Loss: 0.4675\n",
            "Epoch 82/100 - Loss: 0.4674\n",
            "Epoch 83/100 - Loss: 0.4674\n",
            "Epoch 84/100 - Loss: 0.4674\n",
            "Epoch 85/100 - Loss: 0.4673\n",
            "Epoch 86/100 - Loss: 0.4673\n",
            "Epoch 87/100 - Loss: 0.4672\n",
            "Epoch 88/100 - Loss: 0.4673\n",
            "Epoch 89/100 - Loss: 0.4671\n",
            "Epoch 90/100 - Loss: 0.4674\n",
            "Epoch 91/100 - Loss: 0.4671\n",
            "Epoch 92/100 - Loss: 0.4671\n",
            "Epoch 93/100 - Loss: 0.4670\n",
            "Epoch 94/100 - Loss: 0.4669\n",
            "Epoch 95/100 - Loss: 0.4668\n",
            "Epoch 96/100 - Loss: 0.4669\n",
            "Epoch 97/100 - Loss: 0.4668\n",
            "Epoch 98/100 - Loss: 0.4668\n",
            "Epoch 99/100 - Loss: 0.4668\n",
            "Epoch 100/100 - Loss: 0.4669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_proj.eval()\n",
        "text_proj.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_proj = image_proj(img_tensor_pooled)\n",
        "    txt_proj = text_proj(text_tensor)\n",
        "\n",
        "    img_proj_norm = F.normalize(img_proj, dim=1)\n",
        "    txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "    similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)\n",
        "    probs = torch.sigmoid(similarity).cpu().numpy()  # [M, 7]\n",
        "\n",
        "labels_np = labels.cpu().numpy()\n",
        "\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (probs > t).astype(int)\n",
        "    f1 = f1_score(labels_np, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\" Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHsx4B4i9Yyk",
        "outputId": "af3df86f-7c53-4274-f264-44fb358e211c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Best Threshold: 0.35 ‚Üí F1: 0.4928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = (probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(labels_np, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(labels_np, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWKSJlNp9b_B",
        "outputId": "dc5914ca-7623-4d96-e9f8-5e6d6de3285c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.69      0.85      0.76        80\n",
            "    Cardiomegaly       0.59      0.53      0.56        68\n",
            "   Consolidation       0.51      0.85      0.64        33\n",
            "           Edema       0.50      0.56      0.53        45\n",
            "Pleural Effusion       0.67      0.87      0.75        67\n",
            "       Pneumonia       0.10      0.50      0.17         8\n",
            "    Pneumothorax       0.03      0.12      0.05         8\n",
            "\n",
            "       micro avg       0.52      0.71      0.60       309\n",
            "       macro avg       0.44      0.61      0.49       309\n",
            "    weighted avg       0.58      0.71      0.63       309\n",
            "     samples avg       0.23      0.35      0.26       309\n",
            "\n",
            " Hamming Loss: 0.1807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMdawLOmyFdd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrastive Learning"
      ],
      "metadata": {
        "id": "h-Vf8X_ayJew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(image_features, text_features, temperature=0.2):\n",
        "    \"\"\"\n",
        "    image_features: [B, D]\n",
        "    text_features: [B, D]\n",
        "    Returns:\n",
        "        scalar contrastive loss between image and its positive label embedding\n",
        "    \"\"\"\n",
        "    image_features = F.normalize(image_features, dim=1)\n",
        "    text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "    logits = torch.matmul(image_features, text_features.T) / temperature\n",
        "    targets = torch.arange(image_features.size(0)).to(image_features.device)\n",
        "\n",
        "    loss_i = F.cross_entropy(logits, targets)\n",
        "    loss_t = F.cross_entropy(logits.T, targets)\n",
        "\n",
        "    return (loss_i + loss_t) / 2\n"
      ],
      "metadata": {
        "id": "SNgSPYG8yItD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positive_text_embeddings(labels_batch, text_tensor):\n",
        "    \"\"\"\n",
        "    For each image in batch, select one of its positive labels randomly.\n",
        "    Returns: [B, 768] ‚Üí input to text_proj\n",
        "    \"\"\"\n",
        "    B = labels_batch.size(0)\n",
        "    selected_texts = []\n",
        "\n",
        "    for i in range(B):\n",
        "        positive_indices = torch.nonzero(labels_batch[i]).squeeze(1)\n",
        "\n",
        "        if len(positive_indices) == 0:\n",
        "            idx = torch.tensor(0).to(labels_batch.device)\n",
        "        else:\n",
        "            idx = positive_indices[torch.randint(len(positive_indices), (1,)).item()]\n",
        "\n",
        "        selected_texts.append(text_tensor[idx])\n",
        "\n",
        "    return torch.stack(selected_texts)  # [B, 768]\n"
      ],
      "metadata": {
        "id": "o15iOKetyWcK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(img_tensor_pooled, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "F1uplfr2yZ12"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_proj = nn.Sequential(\n",
        "    nn.Linear(64, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ").to(device)\n",
        "\n",
        "text_proj = nn.Sequential(\n",
        "    nn.Linear(768, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "MZouc_YUyb3B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    list(image_proj.parameters()) + list(text_proj.parameters()), lr=5e-4\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    image_proj.train()\n",
        "    text_proj.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for img_batch, label_batch in train_loader:\n",
        "        img_batch = img_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward: image ‚Üí projected\n",
        "        img_proj = image_proj(img_batch)  # [B, 256]\n",
        "\n",
        "        # Text embeddings for positive labels\n",
        "        selected_txt_embed = get_positive_text_embeddings(label_batch, text_tensor)  # [B, 768]\n",
        "        txt_proj = text_proj(selected_txt_embed)  # [B, 256]\n",
        "\n",
        "        # Contrastive loss\n",
        "        loss = contrastive_loss(img_proj, txt_proj)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * img_batch.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs6DUZ8Yyeio",
        "outputId": "aa5de45c-1311-4229-b8c2-2704937a2634"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 4.0931\n",
            "Epoch 2/100 - Loss: 4.0540\n",
            "Epoch 3/100 - Loss: 4.0370\n",
            "Epoch 4/100 - Loss: 3.9910\n",
            "Epoch 5/100 - Loss: 3.9271\n",
            "Epoch 6/100 - Loss: 3.9795\n",
            "Epoch 7/100 - Loss: 3.9844\n",
            "Epoch 8/100 - Loss: 3.9322\n",
            "Epoch 9/100 - Loss: 3.9365\n",
            "Epoch 10/100 - Loss: 3.9613\n",
            "Epoch 11/100 - Loss: 3.9459\n",
            "Epoch 12/100 - Loss: 3.9199\n",
            "Epoch 13/100 - Loss: 3.9081\n",
            "Epoch 14/100 - Loss: 3.9123\n",
            "Epoch 15/100 - Loss: 3.9083\n",
            "Epoch 16/100 - Loss: 3.9154\n",
            "Epoch 17/100 - Loss: 3.9120\n",
            "Epoch 18/100 - Loss: 3.8964\n",
            "Epoch 19/100 - Loss: 3.8614\n",
            "Epoch 20/100 - Loss: 3.8923\n",
            "Epoch 21/100 - Loss: 3.8558\n",
            "Epoch 22/100 - Loss: 3.8542\n",
            "Epoch 23/100 - Loss: 3.8537\n",
            "Epoch 24/100 - Loss: 3.8579\n",
            "Epoch 25/100 - Loss: 3.8146\n",
            "Epoch 26/100 - Loss: 3.8953\n",
            "Epoch 27/100 - Loss: 3.8358\n",
            "Epoch 28/100 - Loss: 3.7985\n",
            "Epoch 29/100 - Loss: 3.8132\n",
            "Epoch 30/100 - Loss: 3.8329\n",
            "Epoch 31/100 - Loss: 3.8346\n",
            "Epoch 32/100 - Loss: 3.8066\n",
            "Epoch 33/100 - Loss: 3.8177\n",
            "Epoch 34/100 - Loss: 3.7838\n",
            "Epoch 35/100 - Loss: 3.8370\n",
            "Epoch 36/100 - Loss: 3.8993\n",
            "Epoch 37/100 - Loss: 3.8201\n",
            "Epoch 38/100 - Loss: 3.8418\n",
            "Epoch 39/100 - Loss: 3.7915\n",
            "Epoch 40/100 - Loss: 3.8317\n",
            "Epoch 41/100 - Loss: 3.8105\n",
            "Epoch 42/100 - Loss: 3.8439\n",
            "Epoch 43/100 - Loss: 3.8154\n",
            "Epoch 44/100 - Loss: 3.7927\n",
            "Epoch 45/100 - Loss: 3.8293\n",
            "Epoch 46/100 - Loss: 3.8427\n",
            "Epoch 47/100 - Loss: 3.7842\n",
            "Epoch 48/100 - Loss: 3.8306\n",
            "Epoch 49/100 - Loss: 3.7975\n",
            "Epoch 50/100 - Loss: 3.7741\n",
            "Epoch 51/100 - Loss: 3.7922\n",
            "Epoch 52/100 - Loss: 3.7829\n",
            "Epoch 53/100 - Loss: 3.8361\n",
            "Epoch 54/100 - Loss: 3.7555\n",
            "Epoch 55/100 - Loss: 3.7993\n",
            "Epoch 56/100 - Loss: 3.7530\n",
            "Epoch 57/100 - Loss: 3.7728\n",
            "Epoch 58/100 - Loss: 3.8322\n",
            "Epoch 59/100 - Loss: 3.7704\n",
            "Epoch 60/100 - Loss: 3.8301\n",
            "Epoch 61/100 - Loss: 3.8042\n",
            "Epoch 62/100 - Loss: 3.7972\n",
            "Epoch 63/100 - Loss: 3.7983\n",
            "Epoch 64/100 - Loss: 3.7846\n",
            "Epoch 65/100 - Loss: 3.7954\n",
            "Epoch 66/100 - Loss: 3.7904\n",
            "Epoch 67/100 - Loss: 3.7289\n",
            "Epoch 68/100 - Loss: 3.8351\n",
            "Epoch 69/100 - Loss: 3.7901\n",
            "Epoch 70/100 - Loss: 3.7775\n",
            "Epoch 71/100 - Loss: 3.7408\n",
            "Epoch 72/100 - Loss: 3.7855\n",
            "Epoch 73/100 - Loss: 3.7663\n",
            "Epoch 74/100 - Loss: 3.7703\n",
            "Epoch 75/100 - Loss: 3.7438\n",
            "Epoch 76/100 - Loss: 3.7729\n",
            "Epoch 77/100 - Loss: 3.7582\n",
            "Epoch 78/100 - Loss: 3.7941\n",
            "Epoch 79/100 - Loss: 3.7691\n",
            "Epoch 80/100 - Loss: 3.7525\n",
            "Epoch 81/100 - Loss: 3.7455\n",
            "Epoch 82/100 - Loss: 3.7503\n",
            "Epoch 83/100 - Loss: 3.8319\n",
            "Epoch 84/100 - Loss: 3.7716\n",
            "Epoch 85/100 - Loss: 3.7595\n",
            "Epoch 86/100 - Loss: 3.7259\n",
            "Epoch 87/100 - Loss: 3.7804\n",
            "Epoch 88/100 - Loss: 3.7952\n",
            "Epoch 89/100 - Loss: 3.6838\n",
            "Epoch 90/100 - Loss: 3.7395\n",
            "Epoch 91/100 - Loss: 3.8070\n",
            "Epoch 92/100 - Loss: 3.7469\n",
            "Epoch 93/100 - Loss: 3.7054\n",
            "Epoch 94/100 - Loss: 3.7171\n",
            "Epoch 95/100 - Loss: 3.7576\n",
            "Epoch 96/100 - Loss: 3.7044\n",
            "Epoch 97/100 - Loss: 3.8140\n",
            "Epoch 98/100 - Loss: 3.7365\n",
            "Epoch 99/100 - Loss: 3.7118\n",
            "Epoch 100/100 - Loss: 3.7485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_proj.eval()\n",
        "text_proj.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_proj_all = F.normalize(image_proj(img_tensor_pooled), dim=1)  # [M, 256]\n",
        "    txt_proj_all = F.normalize(text_proj(text_tensor), dim=1)         # [7, 256]\n",
        "\n",
        "    similarity = torch.matmul(img_proj_all, txt_proj_all.T)           # [M, 7]\n",
        "    probs = torch.sigmoid(similarity).cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "Z77uNN_LzDr9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, hamming_loss\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "\n",
        "for t in np.arange(0.1, 0.9, 0.01):\n",
        "    preds_t = (probs > t).astype(int)\n",
        "    f1 = f1_score(labels_np, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"‚úÖ Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")\n",
        "\n",
        "y_pred = (probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(labels_np, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(labels_np, y_pred)\n",
        "print(f\"üîª Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKvXGZG6zMoT",
        "outputId": "4ac8066a-55b0-4bf3-ec37-ab462c8287f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Best Threshold: 0.57 ‚Üí F1: 0.4680\n",
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.14      0.16      0.15        80\n",
            "    Cardiomegaly       0.69      0.56      0.62        68\n",
            "   Consolidation       0.47      0.94      0.63        33\n",
            "           Edema       0.55      0.84      0.67        45\n",
            "Pleural Effusion       0.79      0.84      0.81        67\n",
            "       Pneumonia       0.12      0.88      0.21         8\n",
            "    Pneumothorax       0.11      0.75      0.19         8\n",
            "\n",
            "       micro avg       0.41      0.61      0.49       309\n",
            "       macro avg       0.41      0.71      0.47       309\n",
            "    weighted avg       0.50      0.61      0.53       309\n",
            "     samples avg       0.32      0.39      0.33       309\n",
            "\n",
            "üîª Hamming Loss: 0.2424\n"
          ]
        }
      ]
    }
  ]
}