{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "#Preparing image and text embeddings for zero-shot and contrastive learning __V2\n",
        "# ==========================================\n",
        "# Author: Morvarid Rahbar\n",
        "# Student ID: 4033624008\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "beZMRGYkoLgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGXvpQx3f7Q-",
        "outputId": "43ba4332-d24a-4013-e03d-5220083437a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report, hamming_loss, accuracy_score\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\" Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_6WbFSHf_n8",
        "outputId": "8683e384-2f5e-4e4e-afbd-2d20110dd3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/chexpert_data_v2/valid.csv\")\n",
        "\n",
        "image_embedding_path = \"/content/drive/MyDrive/Embedings/Image_embeding/image_embeddings_LRF30%.pt\"\n",
        "text_embedding_path = \"/content/drive/MyDrive/Embedings/Text_embeding/disease_text_embeddings.pt\"\n",
        "\n",
        "image_embeddings = torch.load(image_embedding_path)  # dict: key = img_path, value = [197, 64]\n",
        "text_embeddings = torch.load(text_embedding_path)    # dict: key = class name, value = [768]\n",
        "\n"
      ],
      "metadata": {
        "id": "nsG18E9igN6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_columns = [\n",
        "    'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
        "    'Edema', 'Pleural Effusion', 'Pneumonia', 'Pneumothorax'\n",
        "]\n",
        "\n",
        "label_dict = {}\n",
        "prefix = \"CheXpert-v1.0-small/\"\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    path = row[\"Path\"]\n",
        "    if path.startswith(prefix):\n",
        "        path = path[len(prefix):]\n",
        "\n",
        "    label = []\n",
        "    for disease in disease_columns:\n",
        "        val = row[disease]\n",
        "        label.append(0.0 if pd.isna(val) else float(val))\n",
        "\n",
        "    label_dict[path] = torch.tensor(label)\n"
      ],
      "metadata": {
        "id": "PWnaZUwqgQr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ÿ≥ÿßÿÆÿ™ ÿ™ŸÜÿ≥Ÿàÿ± ÿßŸÖÿ®ÿØ€åŸÜ⁄Ø‚ÄåŸáÿß Ÿà ŸÑ€åÿ®ŸÑ‚ÄåŸáÿß ---\n",
        "img_keys = list(image_embeddings.keys())\n",
        "img_tensor = torch.stack([image_embeddings[k] for k in img_keys])         # [N, 197, 64]\n",
        "img_tensor_pooled = img_tensor.max(dim=1).values  # [N, 64]\n",
        "\n",
        "labels_list = []\n",
        "valid_indices = []\n",
        "for idx, k in enumerate(img_keys):\n",
        "    label = label_dict[k]\n",
        "    if (label == -1).all():\n",
        "\n",
        "        continue\n",
        "    else:\n",
        "        label = torch.where(label == -1, torch.tensor(1.0), label)\n",
        "        labels_list.append(label)\n",
        "        valid_indices.append(idx)\n",
        "\n",
        "labels = torch.stack(labels_list)  # [M, 7]\n",
        "img_tensor_pooled = img_tensor_pooled[valid_indices]  # [M, 64]\n",
        "\n",
        "\n",
        "text_tensor = torch.stack([text_embeddings[d] for d in disease_columns])  # [7, 768]\n",
        "\n",
        "\n",
        "img_tensor_pooled = img_tensor_pooled.to(device)\n",
        "labels = labels.to(device)\n",
        "text_tensor = text_tensor.to(device)"
      ],
      "metadata": {
        "id": "N_4t8_UUgSnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Project image and text embeddings to shared 256-dim space\n",
        "image_proj = nn.Linear(64, 256).to(device)\n",
        "text_proj = nn.Linear(768, 256).to(device)\n",
        "\n",
        "# Forward projection\n",
        "img_proj = F.normalize(image_proj(img_tensor_pooled), dim=1)     # [234, 256]\n",
        "txt_proj = F.normalize(text_proj(text_tensor), dim=1)     # [7, 256]\n",
        "\n",
        "# Similarity: [234, 7]\n",
        "similarity = img_proj @ txt_proj.T\n"
      ],
      "metadata": {
        "id": "ycDw4E1YgU6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilities\n",
        "probs = torch.sigmoid(similarity)  # [234, 7]\n",
        "\n",
        "# Threshold = 0.5\n",
        "threshold = 0.8\n",
        "preds = (probs > threshold).float()\n",
        "\n",
        "# Accuracy (Exact Match)\n",
        "exact_match = (preds == labels).all(dim=1).float().mean()\n",
        "print(f\" Exact Match Accuracy: {exact_match.item():.4f}\")\n",
        "\n",
        "# Sample-wise Accuracy\n",
        "sample_accuracy = (preds == labels).float().mean()\n",
        "print(f\" Sample-wise Accuracy: {sample_accuracy.item():.4f}\")\n",
        "\n",
        "# Per-label Accuracy\n",
        "per_label_acc = (preds == labels).float().mean(dim=0)\n",
        "for i, disease in enumerate(disease_columns):\n",
        "    print(f\"{disease}: {per_label_acc[i].item():.4f}\")\n",
        "\n",
        "# Macro Accuracy\n",
        "macro_accuracy = per_label_acc.mean()\n",
        "print(f\" Macro (Mean Per-Label) Accuracy: {macro_accuracy.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpZFzQbqgWx-",
        "outputId": "d71659fe-d4be-4a37-cd7f-287ace6bfff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exact Match Accuracy: 0.3889\n",
            "‚úÖ Sample-wise Accuracy: 0.8114\n",
            "Atelectasis: 0.6581\n",
            "Cardiomegaly: 0.7094\n",
            "Consolidation: 0.8590\n",
            "Edema: 0.8077\n",
            "Pleural Effusion: 0.7137\n",
            "Pneumonia: 0.9658\n",
            "Pneumothorax: 0.9658\n",
            "‚úÖ Macro (Mean Per-Label) Accuracy: 0.8114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels.cpu().numpy()\n",
        "y_prob = probs.detach().cpu().numpy()\n",
        "\n",
        "# Threshold Tuning\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (y_prob > t).astype(int)\n",
        "    f1 = f1_score(y_true, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"üîß Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")\n",
        "\n",
        "# Final Predictions with best threshold\n",
        "y_pred = (y_prob > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UBMYniugZix",
        "outputId": "f4981035-42bb-4ccb-ea7b-76e77b4a8d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Best Threshold: 0.10 ‚Üí F1: 0.3010\n",
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.34      1.00      0.51        80\n",
            "    Cardiomegaly       0.29      1.00      0.45        68\n",
            "   Consolidation       0.14      1.00      0.25        33\n",
            "           Edema       0.19      1.00      0.32        45\n",
            "Pleural Effusion       0.29      1.00      0.45        67\n",
            "       Pneumonia       0.03      1.00      0.07         8\n",
            "    Pneumothorax       0.03      1.00      0.07         8\n",
            "\n",
            "       micro avg       0.19      1.00      0.32       309\n",
            "       macro avg       0.19      1.00      0.30       309\n",
            "    weighted avg       0.26      1.00      0.40       309\n",
            "     samples avg       0.19      0.61      0.27       309\n",
            "\n",
            "‚ùå Hamming Loss: 0.8114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrastive learning"
      ],
      "metadata": {
        "id": "S5lQI6-Swtii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(list(image_proj.parameters()) + list(text_proj.parameters()), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "dataset_size = img_tensor_pooled.size(0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(dataset_size)\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        indices = perm[i:i+batch_size]\n",
        "\n",
        "        img_batch = img_tensor_pooled[indices]   # [B, 64]\n",
        "        label_batch = labels[indices]             # [B, 7]\n",
        "\n",
        "        img_proj_batch = image_proj(img_batch)   # [B, 256]\n",
        "        txt_proj = text_proj(text_tensor)        # [7, 256]\n",
        "\n",
        "        img_proj_norm = F.normalize(img_proj_batch, dim=1)\n",
        "        txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "        similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)  # [B, 7]\n",
        "\n",
        "        loss = criterion(similarity, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * img_batch.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / dataset_size\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuSlFOGwyDG",
        "outputId": "374fc74a-1233-4638-b8e9-5417971f71c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 0.4722\n",
            "Epoch 2/100 - Loss: 0.4732\n",
            "Epoch 3/100 - Loss: 0.4705\n",
            "Epoch 4/100 - Loss: 0.4698\n",
            "Epoch 5/100 - Loss: 0.4699\n",
            "Epoch 6/100 - Loss: 0.4683\n",
            "Epoch 7/100 - Loss: 0.4685\n",
            "Epoch 8/100 - Loss: 0.4679\n",
            "Epoch 9/100 - Loss: 0.4680\n",
            "Epoch 10/100 - Loss: 0.4680\n",
            "Epoch 11/100 - Loss: 0.4678\n",
            "Epoch 12/100 - Loss: 0.4676\n",
            "Epoch 13/100 - Loss: 0.4678\n",
            "Epoch 14/100 - Loss: 0.4675\n",
            "Epoch 15/100 - Loss: 0.4683\n",
            "Epoch 16/100 - Loss: 0.4687\n",
            "Epoch 17/100 - Loss: 0.4684\n",
            "Epoch 18/100 - Loss: 0.4678\n",
            "Epoch 19/100 - Loss: 0.4678\n",
            "Epoch 20/100 - Loss: 0.4677\n",
            "Epoch 21/100 - Loss: 0.4672\n",
            "Epoch 22/100 - Loss: 0.4681\n",
            "Epoch 23/100 - Loss: 0.4682\n",
            "Epoch 24/100 - Loss: 0.4672\n",
            "Epoch 25/100 - Loss: 0.4678\n",
            "Epoch 26/100 - Loss: 0.4673\n",
            "Epoch 27/100 - Loss: 0.4672\n",
            "Epoch 28/100 - Loss: 0.4671\n",
            "Epoch 29/100 - Loss: 0.4671\n",
            "Epoch 30/100 - Loss: 0.4669\n",
            "Epoch 31/100 - Loss: 0.4669\n",
            "Epoch 32/100 - Loss: 0.4669\n",
            "Epoch 33/100 - Loss: 0.4668\n",
            "Epoch 34/100 - Loss: 0.4670\n",
            "Epoch 35/100 - Loss: 0.4667\n",
            "Epoch 36/100 - Loss: 0.4669\n",
            "Epoch 37/100 - Loss: 0.4667\n",
            "Epoch 38/100 - Loss: 0.4666\n",
            "Epoch 39/100 - Loss: 0.4666\n",
            "Epoch 40/100 - Loss: 0.4665\n",
            "Epoch 41/100 - Loss: 0.4666\n",
            "Epoch 42/100 - Loss: 0.4666\n",
            "Epoch 43/100 - Loss: 0.4664\n",
            "Epoch 44/100 - Loss: 0.4664\n",
            "Epoch 45/100 - Loss: 0.4664\n",
            "Epoch 46/100 - Loss: 0.4662\n",
            "Epoch 47/100 - Loss: 0.4664\n",
            "Epoch 48/100 - Loss: 0.4664\n",
            "Epoch 49/100 - Loss: 0.4664\n",
            "Epoch 50/100 - Loss: 0.4663\n",
            "Epoch 51/100 - Loss: 0.4660\n",
            "Epoch 52/100 - Loss: 0.4661\n",
            "Epoch 53/100 - Loss: 0.4660\n",
            "Epoch 54/100 - Loss: 0.4658\n",
            "Epoch 55/100 - Loss: 0.4659\n",
            "Epoch 56/100 - Loss: 0.4659\n",
            "Epoch 57/100 - Loss: 0.4658\n",
            "Epoch 58/100 - Loss: 0.4656\n",
            "Epoch 59/100 - Loss: 0.4659\n",
            "Epoch 60/100 - Loss: 0.4657\n",
            "Epoch 61/100 - Loss: 0.4654\n",
            "Epoch 62/100 - Loss: 0.4655\n",
            "Epoch 63/100 - Loss: 0.4653\n",
            "Epoch 64/100 - Loss: 0.4654\n",
            "Epoch 65/100 - Loss: 0.4654\n",
            "Epoch 66/100 - Loss: 0.4654\n",
            "Epoch 67/100 - Loss: 0.4661\n",
            "Epoch 68/100 - Loss: 0.4655\n",
            "Epoch 69/100 - Loss: 0.4651\n",
            "Epoch 70/100 - Loss: 0.4653\n",
            "Epoch 71/100 - Loss: 0.4650\n",
            "Epoch 72/100 - Loss: 0.4655\n",
            "Epoch 73/100 - Loss: 0.4653\n",
            "Epoch 74/100 - Loss: 0.4651\n",
            "Epoch 75/100 - Loss: 0.4654\n",
            "Epoch 76/100 - Loss: 0.4650\n",
            "Epoch 77/100 - Loss: 0.4651\n",
            "Epoch 78/100 - Loss: 0.4649\n",
            "Epoch 79/100 - Loss: 0.4648\n",
            "Epoch 80/100 - Loss: 0.4650\n",
            "Epoch 81/100 - Loss: 0.4648\n",
            "Epoch 82/100 - Loss: 0.4645\n",
            "Epoch 83/100 - Loss: 0.4647\n",
            "Epoch 84/100 - Loss: 0.4652\n",
            "Epoch 85/100 - Loss: 0.4649\n",
            "Epoch 86/100 - Loss: 0.4648\n",
            "Epoch 87/100 - Loss: 0.4645\n",
            "Epoch 88/100 - Loss: 0.4646\n",
            "Epoch 89/100 - Loss: 0.4645\n",
            "Epoch 90/100 - Loss: 0.4648\n",
            "Epoch 91/100 - Loss: 0.4647\n",
            "Epoch 92/100 - Loss: 0.4643\n",
            "Epoch 93/100 - Loss: 0.4643\n",
            "Epoch 94/100 - Loss: 0.4643\n",
            "Epoch 95/100 - Loss: 0.4644\n",
            "Epoch 96/100 - Loss: 0.4647\n",
            "Epoch 97/100 - Loss: 0.4647\n",
            "Epoch 98/100 - Loss: 0.4648\n",
            "Epoch 99/100 - Loss: 0.4647\n",
            "Epoch 100/100 - Loss: 0.4643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_proj.eval()\n",
        "text_proj.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_proj = image_proj(img_tensor_pooled)\n",
        "    txt_proj = text_proj(text_tensor)\n",
        "\n",
        "    img_proj_norm = F.normalize(img_proj, dim=1)\n",
        "    txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "    similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)\n",
        "    probs = torch.sigmoid(similarity).cpu().numpy()  # [M, 7]\n",
        "\n",
        "labels_np = labels.cpu().numpy()\n",
        "\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (probs > t).astype(int)\n",
        "    f1 = f1_score(labels_np, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\" Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHsx4B4i9Yyk",
        "outputId": "73c51e84-c9c7-4fd7-c727-71d9401ea142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Best Threshold: 0.35 ‚Üí F1: 0.5064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = (probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(labels_np, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(labels_np, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWKSJlNp9b_B",
        "outputId": "958815a8-ce62-44f8-86ca-c9527a162856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.71      0.84      0.77        80\n",
            "    Cardiomegaly       0.60      0.59      0.59        68\n",
            "   Consolidation       0.51      0.88      0.64        33\n",
            "           Edema       0.48      0.53      0.51        45\n",
            "Pleural Effusion       0.70      0.81      0.75        67\n",
            "       Pneumonia       0.15      0.62      0.24         8\n",
            "    Pneumothorax       0.03      0.12      0.05         8\n",
            "\n",
            "       micro avg       0.53      0.71      0.61       309\n",
            "       macro avg       0.45      0.63      0.51       309\n",
            "    weighted avg       0.59      0.71      0.64       309\n",
            "     samples avg       0.23      0.34      0.26       309\n",
            "\n",
            "‚ùå Hamming Loss: 0.1722\n"
          ]
        }
      ]
    }
  ]
}