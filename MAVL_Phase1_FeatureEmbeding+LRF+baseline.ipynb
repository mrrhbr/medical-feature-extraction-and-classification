{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OV6K0iGoA0JN"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "#  CheXpert + ViT + LRFL - Fine-Tuning Notebook\n",
        "# ==========================================\n",
        "# Training ViT model for low-Rank Feature Learning (LRFL)\n",
        "# Morvarid Rahbar\n",
        "# 4033624008\n",
        "# =========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wWmracy0bbA9"
      },
      "outputs": [],
      "source": [
        "# Built-in Libraries\n",
        "import os\n",
        "import copy\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Image & Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Models\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    f1_score,\n",
        "    hamming_loss,\n",
        "    accuracy_score\n",
        ")\n",
        "\n",
        "# Utilities\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Experiment Tracking\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "c_WtItEF6B7e"
      },
      "outputs": [],
      "source": [
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "oPQr6dsK1bvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7098ce1c-1f44-4451-defe-627b0fc20d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\" Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qDdq3YoesYNw"
      },
      "outputs": [],
      "source": [
        "# LRFL Module\n",
        "class LRFLModel(nn.Module):\n",
        "    def __init__(self, backbone_name='vit_base_patch16_224', rank=64, num_classes=5, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0)\n",
        "        self.embed_dim = self.backbone.num_features\n",
        "\n",
        "        self.low_rank_proj = nn.Sequential(\n",
        "            nn.LayerNorm(self.embed_dim),\n",
        "            nn.Linear(self.embed_dim, rank, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.classifier = nn.Linear(rank, num_classes)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        feats = self.backbone.forward_features(x)\n",
        "        if feats.dim() == 3:  # For ViT-style models\n",
        "            feats = feats[:, 0, :]\n",
        "        return feats\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        feats = self.forward_features(x)\n",
        "        proj = self.low_rank_proj(feats)\n",
        "        logits = self.classifier(proj)\n",
        "        return (logits, feats) if return_feats else logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LCBaZU8QnTRM"
      },
      "outputs": [],
      "source": [
        "# BaseLine Model\n",
        "# class BaseModel(nn.Module):\n",
        "#     def __init__(self, backbone_name='vit_base_patch16_224', num_classes=5):\n",
        "#         super().__init__()\n",
        "#         self.backbone = timm.create_model(backbone_name, pretrained=True)\n",
        "\n",
        "#         in_features = self.backbone.head.in_features\n",
        "#         self.backbone.reset_classifier(0)\n",
        "\n",
        "#         self.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         feats = self.backbone.forward_features(x)\n",
        "\n",
        "#         logits = self.classifier(feats)\n",
        "#         return logits\n",
        "\n",
        "#     def get_image_embedding(self, x):\n",
        "#         feats = self.backbone.forward_features(x)\n",
        "#         return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "y2Qm1_wUHz3v"
      },
      "outputs": [],
      "source": [
        "class CheXpertDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_cols = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Pleural Effusion\"]\n",
        "\n",
        "        self.data[self.label_cols] = self.data[self.label_cols].fillna(0).replace(-1, 1)\n",
        "\n",
        "\n",
        "        self.prefix = \"CheXpert-v1.0-small/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        path = row[\"Path\"]\n",
        "\n",
        "        if path.startswith(self.prefix):\n",
        "            path = path[len(self.prefix):]\n",
        "\n",
        "        image_path = os.path.join(self.root_dir, path)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        labels = row[self.label_cols].values.astype(np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, path  # üëà ŸÅŸÇÿ∑ image Ÿà path ÿ®ÿØŸá (label ŸÜŸÖ€å‚ÄåÿÆŸàÿß€å ÿ®ÿ±ÿß€å embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8TVor3UcKIrX"
      },
      "outputs": [],
      "source": [
        "#  Transforms\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "V2Z68yZrrGHb"
      },
      "outputs": [],
      "source": [
        "# LRFL Regularization\n",
        "def compute_uv(features, rank):\n",
        "    with torch.no_grad():\n",
        "        U, _, Vh = torch.linalg.svd(features, full_matrices=False)\n",
        "        return U[:, :rank], Vh[:rank, :]\n",
        "\n",
        "def lrfl_loss_fn(logits, labels, features, U, V, eta=1e-3):\n",
        "    bce = nn.BCEWithLogitsLoss()(logits, labels)\n",
        "    reg = torch.sum((U.T @ features) @ V.T)\n",
        "    return bce + eta * reg / features.size(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xxQUQv1N_5WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554ae3d5-8651-4f78-e31e-dbf8aa5c163d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Exists: True\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/chexpert_data_v2/train.csv')\n",
        "test_path = df['Path'][0]  # CheXpert-v1.0-small/train/patient00001/study1/view1_frontal.jpg\n",
        "test_path = test_path.replace('CheXpert-v1.0-small/', '')\n",
        "full_path = os.path.join('/content/drive/MyDrive/chexpert_data_v2', test_path)\n",
        "print(\" Exists:\", os.path.exists(full_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VQ28_a3DgnRE"
      },
      "outputs": [],
      "source": [
        "csv_path = \"/content/drive/MyDrive/chexpert_data_v2/train.csv\"\n",
        "img_root = \"/content/drive/MyDrive/chexpert_data_v2\"\n",
        "target_root = \"/content/chexpert_data_v2_selected\"\n",
        "target_root = Path(target_root)\n",
        "target_root.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "subset_df = df.sample(frac=0.3 , random_state= 42).reset_index(drop=True)\n",
        "subset_df.to_csv(\"chexpert_30percent.csv\",index = False)"
      ],
      "metadata": {
        "id": "i2kCAJcyABYq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hIoVSoefbP0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f71d6e-c85e-4a73-fb58-5691a76182a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üì• Copying files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67024/67024 [04:00<00:00, 278.83it/s]\n"
          ]
        }
      ],
      "source": [
        "def copy_file(rel_path_str):\n",
        "    prefix = \"CheXpert-v1.0-small/\"\n",
        "    if rel_path_str.startswith(prefix):\n",
        "        rel_path_str = rel_path_str[len(prefix):]\n",
        "    rel_path = Path(rel_path_str)\n",
        "    src = Path(img_root) / rel_path\n",
        "    dst = target_root / rel_path\n",
        "    try:\n",
        "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "    except:\n",
        "        return rel_path\n",
        "\n",
        "image_paths = subset_df[\"Path\"].tolist()\n",
        "with ThreadPoolExecutor(max_workers=32) as executor:\n",
        "    list(tqdm(executor.map(copy_file, image_paths), total=len(image_paths), desc=\"üì• Copying files\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tlVbI6rq8EY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336f8fef-efcb-4092-abe9-383d1c80db7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train  train.csv  valid  valid.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/chexpert_data_v2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "uJZq1u-qfByl"
      },
      "outputs": [],
      "source": [
        "# wandb.init(\n",
        "#     project=\"chexpert-lrf-vit\",\n",
        "#     name=\"run-vit-lrf-v1\",\n",
        "#     config={\n",
        "#         \"lr\": 1e-4,\n",
        "#         \"batch_size\": 128,\n",
        "#         \"epochs\": 10,\n",
        "#         \"model\": \"ViT + LRF\",\n",
        "#         \"rank\": 64\n",
        "#     }\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "aOVp54Et71b6"
      },
      "outputs": [],
      "source": [
        "#   One Epoch Training\n",
        "def train_one_epoch(model, dataloader, optimizer, device, rank, eta):\n",
        "    model.train()\n",
        "    all_preds, all_labels = [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, feats = model(images, return_feats=True)\n",
        "\n",
        "        U, V = compute_uv(feats, rank)\n",
        "        loss = lrfl_loss_fn(logits, labels, feats, U, V, eta)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        all_preds.append(torch.sigmoid(logits).detach().cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds).numpy()\n",
        "    trues = torch.cat(all_labels).numpy()\n",
        "    auc = roc_auc_score(trues, preds, average=\"macro\")\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return auc, avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "cf49KPIUb8Fg"
      },
      "outputs": [],
      "source": [
        "#   Validation\n",
        "def evaluate(model, dataloader, device, return_loss=False, rank=32, eta=1e-3):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits, feats = model(images, return_feats=True)\n",
        "            U, V = compute_uv(feats, rank)\n",
        "            loss = lrfl_loss_fn(logits, labels, feats, U, V, eta)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_preds.append(torch.sigmoid(logits).cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds).numpy()  # [N, C]\n",
        "    trues = torch.cat(all_labels).numpy()  # [N, C]\n",
        "    bin_preds = (preds > 0.5).astype(int)  # Thresholding\n",
        "\n",
        "    # ===== AUC per class =====\n",
        "    aucs = []\n",
        "    for i in range(trues.shape[1]):\n",
        "        try:\n",
        "            auc = roc_auc_score(trues[:, i], preds[:, i])\n",
        "            aucs.append(auc)\n",
        "        except ValueError:\n",
        "            aucs.append(np.nan)  # skip classes with only one label\n",
        "\n",
        "    mean_auc = np.nanmean(aucs)\n",
        "\n",
        "    # ===== Other metrics =====\n",
        "    f1_macro = f1_score(trues, bin_preds, average='macro', zero_division=0)\n",
        "    f1_micro = f1_score(trues, bin_preds, average='micro', zero_division=0)\n",
        "    hamming = hamming_loss(trues, bin_preds)\n",
        "    acc = accuracy_score(trues, bin_preds)  # not very meaningful in multilabel\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    print(f\"\\n   Evaluation Metrics:\")\n",
        "    print(f\" -   Mean AUC:      {mean_auc:.4f}\")\n",
        "    print(f\" -   F1 Macro:      {f1_macro:.4f}\")\n",
        "    print(f\" -   F1 Micro:      {f1_micro:.4f}\")\n",
        "    print(f\" -   Hamming Loss:  {hamming:.4f}\")\n",
        "    print(f\" -   Accuracy:      {acc:.4f}\")\n",
        "\n",
        "    if return_loss:\n",
        "        return mean_auc, avg_loss, f1_macro, hamming\n",
        "    return mean_auc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CxGcQWZX9uIQ"
      },
      "outputs": [],
      "source": [
        "# # ŸÜŸÖÿß€åÿ¥ 5 ŸÜŸÖŸàŸÜŸá‚Äå€å ÿ™ÿµÿßÿØŸÅ€å ÿßÿ≤ train_dataset\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for i in range(5):\n",
        "#     img, label, path = train_dataset[i]\n",
        "#     plt.imshow(img.permute(1, 2, 0))  # ÿ®ÿ±ÿß€å ŸÜŸÖÿß€åÿ¥ [C,H,W] ‚Üí [H,W,C]\n",
        "#     plt.title(f\"Path: {path}\\nLabels: {label}\")\n",
        "#     plt.axis('off')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_BeDuRCr4XgC"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, device, num_epochs=30,\n",
        "          rank=32, eta=1e-3, patience=5, checkpoint_path=\"best_model.pth\", monitor=\"val_loss\"):\n",
        "\n",
        "    # ŸÖŸÇÿØÿßÿ± ÿßŸàŸÑ€åŸá best_metric ÿ®ÿß€åÿØ ÿ®ÿ± ÿßÿ≥ÿßÿ≥ ŸÖÿπ€åÿßÿ± ŸÖŸàŸÜ€åÿ™Ÿàÿ± ÿ™ŸÜÿ∏€åŸÖ ÿ®ÿ¥Ÿá\n",
        "    if monitor in [\"val_loss\", \"hamming\"]:\n",
        "        best_metric = float('inf')\n",
        "    else:\n",
        "        best_metric = -float('inf')  # ÿ®ÿ±ÿß€å ŸÖÿπ€åÿßÿ±Ÿáÿß€å€å ⁄©Ÿá ÿ®ÿ≤ÿ±⁄Øÿ™ÿ± ÿ®Ÿáÿ™ÿ±Ÿá ŸÖÿ´ŸÑ AUC Ÿà F1\n",
        "\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n Epoch {epoch+1}/{num_epochs}\")\n",
        "        train_auc, train_loss = train_one_epoch(model, train_loader, optimizer, device, rank, eta)\n",
        "        val_auc, val_loss, val_f1, val_hamming = evaluate(model, val_loader, device, return_loss=True, rank=rank, eta=eta)\n",
        "\n",
        "        print(f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f} | Val Loss: {val_loss:.4f} | F1 Macro: {val_f1:.4f} | Hamming: {val_hamming:.4f}\")\n",
        "\n",
        "        if monitor == \"val_auc\":\n",
        "            current_metric = val_auc\n",
        "            improvement = current_metric > best_metric\n",
        "        elif monitor == \"val_f1\":\n",
        "            current_metric = val_f1\n",
        "            improvement = current_metric > best_metric\n",
        "        elif monitor == \"hamming\":\n",
        "            current_metric = val_hamming\n",
        "            improvement = current_metric < best_metric\n",
        "        else:  # val_loss\n",
        "            current_metric = val_loss\n",
        "            improvement = current_metric < best_metric\n",
        "\n",
        "        if improvement:\n",
        "            best_metric = current_metric\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(\" Best model saved.\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\" No improvement for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\" Early stopping triggered!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dlgBcM1WYZBD"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = CheXpertDataset(\"chexpert_30percent.csv\", \"/content/chexpert_data_v2_selected\", train_transform)\n",
        "val_dataset   = CheXpertDataset(\"/content/drive/MyDrive/chexpert_data_v2/valid.csv\", \"/content/drive/MyDrive/chexpert_data_v2\", train_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LRFLModel(backbone_name=\"vit_base_patch16_224\", rank=64, num_classes=5).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "5fF4OVS7l0s6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_loader:\n",
        "#     print(\"Batch labels shape:\", labels.shape)\n",
        "#     print(\"First batch of labels:\", labels[:5])\n",
        "#     break\n",
        "# # # for being more strick -1 labels which were mapped to uncertain condition has been transformed to +1 label."
      ],
      "metadata": {
        "id": "AGfzkxrV7qrR"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhZblvqurvv_"
      },
      "outputs": [],
      "source": [
        "# train(model, train_loader, val_loader, optimizer, device,\n",
        "#       num_epochs=20, rank=32, eta=1e-3, patience=5, checkpoint_path=\"best_lrfl_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TU9rzuaW7nf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_loader:\n",
        "#     print(len(batch))\n",
        "#     break"
      ],
      "metadata": {
        "id": "P-OOzMb1yImf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "2xRvGevMgm_Y"
      },
      "outputs": [],
      "source": [
        "# # Extract image embeddings and save\n",
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_lrfl_model_v2.pth\"))\n",
        "# model.eval()\n",
        "\n",
        "# image_embeddings = {}\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for images, paths in tqdm(train_loader):\n",
        "#         images = images.to(device)\n",
        "#         embs = model.get_image_embedding(images)  # (B, R)\n",
        "\n",
        "#         for path, emb in zip(paths, embs):\n",
        "#             image_embeddings[path] = emb.cpu()\n",
        "\n",
        "# torch.save(image_embeddings, \"/content/drive/MyDrive/image_embeddings_LRF30_v2.pt\")\n",
        "# print(\" Image embeddings saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LRFLModel(rank=64, num_classes=5)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_lrfl_model_v2.pth\"))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "image_embeddings = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, paths in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        feats = model.forward_features(images)       # [B, embed_dim]\n",
        "        embs = model.low_rank_proj(feats)            # [B, rank]\n",
        "\n",
        "        for path, emb in zip(paths, embs):\n",
        "            image_embeddings[path] = emb.cpu()\n",
        "\n",
        "\n",
        "torch.save(image_embeddings, \"/content/drive/MyDrive/image_embeddings_LRF30_v2.pt\")\n",
        "print(\"‚úÖ Image embeddings saved successfully!\")\n"
      ],
      "metadata": {
        "id": "d5YGWRzwzODM",
        "outputId": "f5710401-eeeb-406d-bbc1-547b7173254e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4189/4189 [14:50<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Image embeddings saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3jK15v_Xssl"
      },
      "outputs": [],
      "source": [
        "# Flush GPU Memory\n",
        "def clear_cuda_cache():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "clear_cuda_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NcqSTxD3ht-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}