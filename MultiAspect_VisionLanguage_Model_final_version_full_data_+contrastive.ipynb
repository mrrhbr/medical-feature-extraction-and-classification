{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "#Preparing image and text embeddings for zero-shot and contrastive learning __V2\n",
        "# ==========================================\n",
        "# Author: Morvarid Rahbar\n",
        "# Student ID: 4033624008\n",
        "# =========================================="
      ],
      "metadata": {
        "id": "beZMRGYkoLgk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGXvpQx3f7Q-",
        "outputId": "704a474a-f533-4b35-8ae9-80a89ce37cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, classification_report, hamming_loss, accuracy_score\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\" Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_6WbFSHf_n8",
        "outputId": "52c715fe-7d3e-4625-adab-f2ba912d9112"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/chexpert_data_v2/train.csv\")\n",
        "\n",
        "image_embedding_path = \"/content/drive/MyDrive/image_embeddings_LRF30_fulltrainversion%.pt\"\n",
        "text_embedding_path = \"/content/drive/MyDrive/Embedings/Text_embeding/disease_text_embeddings.pt\"\n",
        "\n",
        "image_embeddings = torch.load(image_embedding_path)  # dict: key = img_path, value = [197, 64]\n",
        "text_embeddings = torch.load(text_embedding_path)    # dict: key = class name, value = [768]\n",
        "\n"
      ],
      "metadata": {
        "id": "nsG18E9igN6D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_columns = [\n",
        "    'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
        "    'Edema', 'Pleural Effusion', 'Pneumonia', 'Pneumothorax'\n",
        "]\n",
        "\n",
        "label_dict = {}\n",
        "prefix = \"CheXpert-v1.0-small/\"\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    path = row[\"Path\"]\n",
        "    if path.startswith(prefix):\n",
        "        path = path[len(prefix):]\n",
        "\n",
        "    label = []\n",
        "    for disease in disease_columns:\n",
        "        val = row[disease]\n",
        "        label.append(0.0 if pd.isna(val) else float(val))\n",
        "\n",
        "    label_dict[path] = torch.tensor(label)\n"
      ],
      "metadata": {
        "id": "PWnaZUwqgQr3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ÿ≥ÿßÿÆÿ™ ÿ™ŸÜÿ≥Ÿàÿ± ÿßŸÖÿ®ÿØ€åŸÜ⁄Ø‚ÄåŸáÿß Ÿà ŸÑ€åÿ®ŸÑ‚ÄåŸáÿß ---\n",
        "img_keys = list(image_embeddings.keys())\n",
        "img_tensor = torch.stack([image_embeddings[k] for k in img_keys])         # [N, 197, 64]\n",
        "img_tensor_pooled = img_tensor.max(dim=1).values  # [N, 64]\n",
        "\n",
        "labels_list = []\n",
        "valid_indices = []\n",
        "for idx, k in enumerate(img_keys):\n",
        "    label = label_dict[k]\n",
        "    if (label == -1).all():\n",
        "\n",
        "        continue\n",
        "    else:\n",
        "        label = torch.where(label == -1, torch.tensor(0.0), label)\n",
        "        labels_list.append(label)\n",
        "        valid_indices.append(idx)\n",
        "\n",
        "labels = torch.stack(labels_list)  # [M, 7]\n",
        "img_tensor_pooled = img_tensor_pooled[valid_indices]  # [M, 64]\n",
        "\n",
        "\n",
        "text_tensor = torch.stack([text_embeddings[d] for d in disease_columns])  # [7, 768]\n",
        "\n",
        "\n",
        "img_tensor_pooled = img_tensor_pooled.to(device)\n",
        "labels = labels.to(device)\n",
        "text_tensor = text_tensor.to(device)"
      ],
      "metadata": {
        "id": "N_4t8_UUgSnT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Based Classification"
      ],
      "metadata": {
        "id": "R90anf5_h7ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_tensor shape: [B, 64]\n",
        "image_proj = nn.Sequential(\n",
        "    nn.Linear(64, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ")\n",
        "\n",
        "text_proj = nn.Sequential(\n",
        "    nn.Linear(768, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ")\n",
        "\n",
        "\n",
        "# Apply projection and normalize\n",
        "img_proj = F.normalize(image_proj(img_tensor_pooled), dim=1)  # [B, 256]\n",
        "txt_proj = F.normalize(text_proj(text_tensor), dim=1)         # [7, 256]\n",
        "\n",
        "# Similarity calculation: [B, 7]\n",
        "temperature = nn.Parameter(torch.tensor(0.07))\n",
        "similarity = (img_proj @ txt_proj.T) / temperature\n",
        "\n"
      ],
      "metadata": {
        "id": "ycDw4E1YgU6b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilities\n",
        "probs = torch.sigmoid(similarity)  # [234, 7]\n",
        "\n",
        "# Threshold = 0.5\n",
        "threshold = 0.5\n",
        "preds = (probs > threshold).float()\n",
        "\n",
        "# Accuracy (Exact Match)\n",
        "exact_match = (preds == labels).all(dim=1).float().mean()\n",
        "print(f\" Exact Match Accuracy: {exact_match.item():.4f}\")\n",
        "\n",
        "# Sample-wise Accuracy\n",
        "sample_accuracy = (preds == labels).float().mean()\n",
        "print(f\" Sample-wise Accuracy: {sample_accuracy.item():.4f}\")\n",
        "\n",
        "# Per-label Accuracy\n",
        "per_label_acc = (preds == labels).float().mean(dim=0)\n",
        "for i, disease in enumerate(disease_columns):\n",
        "    print(f\"{disease}: {per_label_acc[i].item():.4f}\")\n",
        "\n",
        "# Macro Accuracy\n",
        "macro_accuracy = per_label_acc.mean()\n",
        "print(f\" Macro (Mean Per-Label) Accuracy: {macro_accuracy.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpZFzQbqgWx-",
        "outputId": "b653826b-9c9c-4aab-df8b-0b98a8636c5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Exact Match Accuracy: 0.0023\n",
            " Sample-wise Accuracy: 0.2624\n",
            "Atelectasis: 0.3923\n",
            "Cardiomegaly: 0.2177\n",
            "Consolidation: 0.1383\n",
            "Edema: 0.3060\n",
            "Pleural Effusion: 0.5149\n",
            "Pneumonia: 0.1520\n",
            "Pneumothorax: 0.1156\n",
            " Macro (Mean Per-Label) Accuracy: 0.2624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = labels.cpu().numpy()\n",
        "y_prob = probs.detach().cpu().numpy()\n",
        "\n",
        "# Threshold Tuning\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (y_prob > t).astype(int)\n",
        "    f1 = f1_score(y_true, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"üîß Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")\n",
        "\n",
        "# Final Predictions with best threshold\n",
        "y_pred = (y_prob > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UBMYniugZix",
        "outputId": "f117a4e3-a322-4713-c3ef-c50dbfe5de0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Best Threshold: 0.15 ‚Üí F1: 0.2488\n",
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.15      1.00      0.26      9934\n",
            "    Cardiomegaly       0.12      1.00      0.21      8062\n",
            "   Consolidation       0.07      1.00      0.12      4432\n",
            "           Edema       0.23      1.00      0.38     15602\n",
            "Pleural Effusion       0.38      1.00      0.56     25776\n",
            "       Pneumonia       0.03      1.00      0.05      1792\n",
            "    Pneumothorax       0.09      1.00      0.16      5797\n",
            "\n",
            "       micro avg       0.15      1.00      0.26     71395\n",
            "       macro avg       0.15      1.00      0.25     71395\n",
            "    weighted avg       0.24      1.00      0.37     71395\n",
            "     samples avg       0.15      0.67      0.24     71395\n",
            "\n",
            " Hamming Loss: 0.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BCEW learning"
      ],
      "metadata": {
        "id": "S5lQI6-Swtii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(list(image_proj.parameters()) + list(text_proj.parameters()), lr=5e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "dataset_size = img_tensor_pooled.size(0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(dataset_size)\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        indices = perm[i:i+batch_size]\n",
        "\n",
        "        img_batch = img_tensor_pooled[indices]   # [B, 64]\n",
        "        label_batch = labels[indices]             # [B, 7]\n",
        "\n",
        "        img_proj_batch = image_proj(img_batch)   # [B, 256]\n",
        "        txt_proj = text_proj(text_tensor)        # [7, 256]\n",
        "\n",
        "        img_proj_norm = F.normalize(img_proj_batch, dim=1)\n",
        "        txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "        similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)  # [B, 7]\n",
        "\n",
        "        loss = criterion(similarity, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * img_batch.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / dataset_size\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuSlFOGwyDG",
        "outputId": "9e416072-9a42-400f-9eb9-7e922ba46724"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130 - Loss: 1.0843\n",
            "Epoch 2/130 - Loss: 1.0677\n",
            "Epoch 3/130 - Loss: 1.0649\n",
            "Epoch 4/130 - Loss: 1.0657\n",
            "Epoch 5/130 - Loss: 1.1025\n",
            "Epoch 6/130 - Loss: 1.0901\n",
            "Epoch 7/130 - Loss: 1.0803\n",
            "Epoch 8/130 - Loss: 1.0608\n",
            "Epoch 9/130 - Loss: 1.0568\n",
            "Epoch 10/130 - Loss: 1.0594\n",
            "Epoch 11/130 - Loss: 1.0547\n",
            "Epoch 12/130 - Loss: 1.0558\n",
            "Epoch 13/130 - Loss: 1.0542\n",
            "Epoch 14/130 - Loss: 1.0544\n",
            "Epoch 15/130 - Loss: 1.0532\n",
            "Epoch 16/130 - Loss: 1.0540\n",
            "Epoch 17/130 - Loss: 1.0522\n",
            "Epoch 18/130 - Loss: 1.0521\n",
            "Epoch 19/130 - Loss: 1.0519\n",
            "Epoch 20/130 - Loss: 1.0514\n",
            "Epoch 21/130 - Loss: 1.0508\n",
            "Epoch 22/130 - Loss: 1.0505\n",
            "Epoch 23/130 - Loss: 1.0503\n",
            "Epoch 24/130 - Loss: 1.0502\n",
            "Epoch 25/130 - Loss: 1.0500\n",
            "Epoch 26/130 - Loss: 1.0498\n",
            "Epoch 27/130 - Loss: 1.0493\n",
            "Epoch 28/130 - Loss: 1.0489\n",
            "Epoch 29/130 - Loss: 1.0497\n",
            "Epoch 30/130 - Loss: 1.0490\n",
            "Epoch 31/130 - Loss: 1.0497\n",
            "Epoch 32/130 - Loss: 1.0490\n",
            "Epoch 33/130 - Loss: 1.0488\n",
            "Epoch 34/130 - Loss: 1.0490\n",
            "Epoch 35/130 - Loss: 1.0489\n",
            "Epoch 36/130 - Loss: 1.0485\n",
            "Epoch 37/130 - Loss: 1.0488\n",
            "Epoch 38/130 - Loss: 1.0488\n",
            "Epoch 39/130 - Loss: 1.0482\n",
            "Epoch 40/130 - Loss: 1.0479\n",
            "Epoch 41/130 - Loss: 1.0481\n",
            "Epoch 42/130 - Loss: 1.0483\n",
            "Epoch 43/130 - Loss: 1.0480\n",
            "Epoch 44/130 - Loss: 1.0473\n",
            "Epoch 45/130 - Loss: 1.0478\n",
            "Epoch 46/130 - Loss: 1.0481\n",
            "Epoch 47/130 - Loss: 1.0473\n",
            "Epoch 48/130 - Loss: 1.0473\n",
            "Epoch 49/130 - Loss: 1.0476\n",
            "Epoch 50/130 - Loss: 1.0473\n",
            "Epoch 51/130 - Loss: 1.0474\n",
            "Epoch 52/130 - Loss: 1.0467\n",
            "Epoch 53/130 - Loss: 1.0468\n",
            "Epoch 54/130 - Loss: 1.0471\n",
            "Epoch 55/130 - Loss: 1.0471\n",
            "Epoch 56/130 - Loss: 1.0471\n",
            "Epoch 57/130 - Loss: 1.0469\n",
            "Epoch 58/130 - Loss: 1.0468\n",
            "Epoch 59/130 - Loss: 1.0467\n",
            "Epoch 60/130 - Loss: 1.0462\n",
            "Epoch 61/130 - Loss: 1.0468\n",
            "Epoch 62/130 - Loss: 1.0468\n",
            "Epoch 63/130 - Loss: 1.0466\n",
            "Epoch 64/130 - Loss: 1.0464\n",
            "Epoch 65/130 - Loss: 1.0460\n",
            "Epoch 66/130 - Loss: 1.0461\n",
            "Epoch 67/130 - Loss: 1.0461\n",
            "Epoch 68/130 - Loss: 1.0459\n",
            "Epoch 69/130 - Loss: 1.0460\n",
            "Epoch 70/130 - Loss: 1.0464\n",
            "Epoch 71/130 - Loss: 1.0458\n",
            "Epoch 72/130 - Loss: 1.0456\n",
            "Epoch 73/130 - Loss: 1.0458\n",
            "Epoch 74/130 - Loss: 1.0462\n",
            "Epoch 75/130 - Loss: 1.0456\n",
            "Epoch 76/130 - Loss: 1.0457\n",
            "Epoch 77/130 - Loss: 1.0460\n",
            "Epoch 78/130 - Loss: 1.0457\n",
            "Epoch 79/130 - Loss: 1.0453\n",
            "Epoch 80/130 - Loss: 1.0456\n",
            "Epoch 81/130 - Loss: 1.0453\n",
            "Epoch 82/130 - Loss: 1.0452\n",
            "Epoch 83/130 - Loss: 1.0455\n",
            "Epoch 84/130 - Loss: 1.0450\n",
            "Epoch 85/130 - Loss: 1.0452\n",
            "Epoch 86/130 - Loss: 1.0447\n",
            "Epoch 87/130 - Loss: 1.0449\n",
            "Epoch 88/130 - Loss: 1.0448\n",
            "Epoch 89/130 - Loss: 1.0447\n",
            "Epoch 90/130 - Loss: 1.0450\n",
            "Epoch 91/130 - Loss: 1.0449\n",
            "Epoch 92/130 - Loss: 1.0445\n",
            "Epoch 93/130 - Loss: 1.0447\n",
            "Epoch 94/130 - Loss: 1.0442\n",
            "Epoch 95/130 - Loss: 1.0443\n",
            "Epoch 96/130 - Loss: 1.0439\n",
            "Epoch 97/130 - Loss: 1.0444\n",
            "Epoch 98/130 - Loss: 1.0440\n",
            "Epoch 99/130 - Loss: 1.0443\n",
            "Epoch 100/130 - Loss: 1.0442\n",
            "Epoch 101/130 - Loss: 1.0439\n",
            "Epoch 102/130 - Loss: 1.0439\n",
            "Epoch 103/130 - Loss: 1.0437\n",
            "Epoch 104/130 - Loss: 1.0438\n",
            "Epoch 105/130 - Loss: 1.0438\n",
            "Epoch 106/130 - Loss: 1.0435\n",
            "Epoch 107/130 - Loss: 1.0432\n",
            "Epoch 108/130 - Loss: 1.0432\n",
            "Epoch 109/130 - Loss: 1.0429\n",
            "Epoch 110/130 - Loss: 1.0433\n",
            "Epoch 111/130 - Loss: 1.0432\n",
            "Epoch 112/130 - Loss: 1.0430\n",
            "Epoch 113/130 - Loss: 1.0426\n",
            "Epoch 114/130 - Loss: 1.0428\n",
            "Epoch 115/130 - Loss: 1.0428\n",
            "Epoch 116/130 - Loss: 1.0428\n",
            "Epoch 117/130 - Loss: 1.0424\n",
            "Epoch 118/130 - Loss: 1.0426\n",
            "Epoch 119/130 - Loss: 1.0421\n",
            "Epoch 120/130 - Loss: 1.0422\n",
            "Epoch 121/130 - Loss: 1.0428\n",
            "Epoch 122/130 - Loss: 1.0422\n",
            "Epoch 123/130 - Loss: 1.0423\n",
            "Epoch 124/130 - Loss: 1.0420\n",
            "Epoch 125/130 - Loss: 1.0416\n",
            "Epoch 126/130 - Loss: 1.0418\n",
            "Epoch 127/130 - Loss: 1.0419\n",
            "Epoch 128/130 - Loss: 1.0413\n",
            "Epoch 129/130 - Loss: 1.0422\n",
            "Epoch 130/130 - Loss: 1.0416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_proj.eval()\n",
        "text_proj.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_proj = image_proj(img_tensor_pooled)\n",
        "    txt_proj = text_proj(text_tensor)\n",
        "\n",
        "    img_proj_norm = F.normalize(img_proj, dim=1)\n",
        "    txt_proj_norm = F.normalize(txt_proj, dim=1)\n",
        "\n",
        "    similarity = torch.matmul(img_proj_norm, txt_proj_norm.T)\n",
        "    probs = torch.sigmoid(similarity).cpu().numpy()  # [M, 7]\n",
        "\n",
        "labels_np = labels.cpu().numpy()\n",
        "\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "for t in np.arange(0.1, 0.9, 0.05):\n",
        "    preds_t = (probs > t).astype(int)\n",
        "    f1 = f1_score(labels_np, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\" Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHsx4B4i9Yyk",
        "outputId": "b724b36b-73e0-441d-c80f-515089bf621f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Best Threshold: 0.55 ‚Üí F1: 0.3751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJHjGJOkw35F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = (probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(labels_np, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(labels_np, y_pred)\n",
        "print(f\" Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWKSJlNp9b_B",
        "outputId": "c5c97270-dd2b-414c-c93f-53daca761482"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.24      0.53      0.33      9934\n",
            "    Cardiomegaly       0.36      0.75      0.48      8062\n",
            "   Consolidation       0.12      0.67      0.20      4432\n",
            "           Edema       0.48      0.67      0.56     15602\n",
            "Pleural Effusion       0.69      0.72      0.70     25776\n",
            "       Pneumonia       0.05      0.52      0.09      1792\n",
            "    Pneumothorax       0.17      0.51      0.25      5797\n",
            "\n",
            "       micro avg       0.32      0.66      0.43     71395\n",
            "       macro avg       0.30      0.62      0.38     71395\n",
            "    weighted avg       0.45      0.66      0.51     71395\n",
            "     samples avg       0.24      0.42      0.29     71395\n",
            "\n",
            " Hamming Loss: 0.2660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d8CxWtd1sft",
        "outputId": "90b247f3-1793-44d2-8482-44b5e80b314e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, ..., 1, 0, 0],\n",
              "       [1, 0, 1, ..., 1, 0, 0],\n",
              "       [0, 0, 1, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 1, ..., 0, 1, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrastive learning\n"
      ],
      "metadata": {
        "id": "so3lUFVOhzl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(image_features, text_features, temperature=0.2):\n",
        "    \"\"\"\n",
        "    image_features: [B, D]\n",
        "    text_features: [B, D]\n",
        "    Returns:\n",
        "        scalar contrastive loss between image and its positive label embedding\n",
        "    \"\"\"\n",
        "    image_features = F.normalize(image_features, dim=1)\n",
        "    text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "    logits = torch.matmul(image_features, text_features.T) / temperature\n",
        "    targets = torch.arange(image_features.size(0)).to(image_features.device)\n",
        "\n",
        "    loss_i = F.cross_entropy(logits, targets)\n",
        "    loss_t = F.cross_entropy(logits.T, targets)\n",
        "\n",
        "    return (loss_i + loss_t) / 2\n"
      ],
      "metadata": {
        "id": "EwTvmjMdiSlY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positive_text_embeddings(labels_batch, text_tensor):\n",
        "    \"\"\"\n",
        "    For each image in batch, select one of its positive labels randomly.\n",
        "    Returns: [B, 768] ‚Üí input to text_proj\n",
        "    \"\"\"\n",
        "    B = labels_batch.size(0)\n",
        "    selected_texts = []\n",
        "\n",
        "    for i in range(B):\n",
        "        positive_indices = torch.nonzero(labels_batch[i]).squeeze(1)\n",
        "\n",
        "        if len(positive_indices) == 0:\n",
        "            idx = torch.tensor(0).to(labels_batch.device)\n",
        "        else:\n",
        "            idx = positive_indices[torch.randint(len(positive_indices), (1,)).item()]\n",
        "\n",
        "        selected_texts.append(text_tensor[idx])\n",
        "\n",
        "    return torch.stack(selected_texts)  # [B, 768]\n"
      ],
      "metadata": {
        "id": "ICf3y8aOh3bX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(img_tensor_pooled, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Nm0eKMo1iWmL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_proj = nn.Sequential(\n",
        "    nn.Linear(64, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ").to(device)\n",
        "\n",
        "text_proj = nn.Sequential(\n",
        "    nn.Linear(768, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.LayerNorm(128),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 256),\n",
        "    nn.LayerNorm(256)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "79SHMxNgiZ4T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    list(image_proj.parameters()) + list(text_proj.parameters()), lr=5e-4\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    image_proj.train()\n",
        "    text_proj.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for img_batch, label_batch in train_loader:\n",
        "        img_batch = img_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward: image ‚Üí projected\n",
        "        img_proj = image_proj(img_batch)  # [B, 256]\n",
        "\n",
        "        # Text embeddings for positive labels\n",
        "        selected_txt_embed = get_positive_text_embeddings(label_batch, text_tensor)  # [B, 768]\n",
        "        txt_proj = text_proj(selected_txt_embed)  # [B, 256]\n",
        "\n",
        "        # Contrastive loss\n",
        "        loss = contrastive_loss(img_proj, txt_proj)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * img_batch.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eukC1mIbicL9",
        "outputId": "e756c241-fe5e-4219-9216-fa13b4f95cf3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 3.8717\n",
            "Epoch 2/100 - Loss: 3.8716\n",
            "Epoch 3/100 - Loss: 3.8710\n",
            "Epoch 4/100 - Loss: 3.8716\n",
            "Epoch 5/100 - Loss: 3.8721\n",
            "Epoch 6/100 - Loss: 3.8692\n",
            "Epoch 7/100 - Loss: 3.8688\n",
            "Epoch 8/100 - Loss: 3.8709\n",
            "Epoch 9/100 - Loss: 3.8702\n",
            "Epoch 10/100 - Loss: 3.8712\n",
            "Epoch 11/100 - Loss: 3.8686\n",
            "Epoch 12/100 - Loss: 3.8683\n",
            "Epoch 13/100 - Loss: 3.8702\n",
            "Epoch 14/100 - Loss: 3.8700\n",
            "Epoch 15/100 - Loss: 3.8696\n",
            "Epoch 16/100 - Loss: 3.8696\n",
            "Epoch 17/100 - Loss: 3.8691\n",
            "Epoch 18/100 - Loss: 3.8699\n",
            "Epoch 19/100 - Loss: 3.8697\n",
            "Epoch 20/100 - Loss: 3.8678\n",
            "Epoch 21/100 - Loss: 3.8674\n",
            "Epoch 22/100 - Loss: 3.8683\n",
            "Epoch 23/100 - Loss: 3.8666\n",
            "Epoch 24/100 - Loss: 3.8655\n",
            "Epoch 25/100 - Loss: 3.8676\n",
            "Epoch 26/100 - Loss: 3.8665\n",
            "Epoch 27/100 - Loss: 3.8662\n",
            "Epoch 28/100 - Loss: 3.8653\n",
            "Epoch 29/100 - Loss: 3.8671\n",
            "Epoch 30/100 - Loss: 3.8676\n",
            "Epoch 31/100 - Loss: 3.8682\n",
            "Epoch 32/100 - Loss: 3.8671\n",
            "Epoch 33/100 - Loss: 3.8652\n",
            "Epoch 34/100 - Loss: 3.8653\n",
            "Epoch 35/100 - Loss: 3.8644\n",
            "Epoch 36/100 - Loss: 3.8633\n",
            "Epoch 37/100 - Loss: 3.8659\n",
            "Epoch 38/100 - Loss: 3.8663\n",
            "Epoch 39/100 - Loss: 3.8644\n",
            "Epoch 40/100 - Loss: 3.8642\n",
            "Epoch 41/100 - Loss: 3.8644\n",
            "Epoch 42/100 - Loss: 3.8647\n",
            "Epoch 43/100 - Loss: 3.8670\n",
            "Epoch 44/100 - Loss: 3.8633\n",
            "Epoch 45/100 - Loss: 3.8649\n",
            "Epoch 46/100 - Loss: 3.8664\n",
            "Epoch 47/100 - Loss: 3.8636\n",
            "Epoch 48/100 - Loss: 3.8660\n",
            "Epoch 49/100 - Loss: 3.8658\n",
            "Epoch 50/100 - Loss: 3.8639\n",
            "Epoch 51/100 - Loss: 3.8619\n",
            "Epoch 52/100 - Loss: 3.8634\n",
            "Epoch 53/100 - Loss: 3.8641\n",
            "Epoch 54/100 - Loss: 3.8633\n",
            "Epoch 55/100 - Loss: 3.8671\n",
            "Epoch 56/100 - Loss: 3.8634\n",
            "Epoch 57/100 - Loss: 3.8620\n",
            "Epoch 58/100 - Loss: 3.8642\n",
            "Epoch 59/100 - Loss: 3.8656\n",
            "Epoch 60/100 - Loss: 3.8632\n",
            "Epoch 61/100 - Loss: 3.8649\n",
            "Epoch 62/100 - Loss: 3.8632\n",
            "Epoch 63/100 - Loss: 3.8649\n",
            "Epoch 64/100 - Loss: 3.8649\n",
            "Epoch 65/100 - Loss: 3.8624\n",
            "Epoch 66/100 - Loss: 3.8601\n",
            "Epoch 67/100 - Loss: 3.8618\n",
            "Epoch 68/100 - Loss: 3.8644\n",
            "Epoch 69/100 - Loss: 3.8644\n",
            "Epoch 70/100 - Loss: 3.8605\n",
            "Epoch 71/100 - Loss: 3.8634\n",
            "Epoch 72/100 - Loss: 3.8636\n",
            "Epoch 73/100 - Loss: 3.8645\n",
            "Epoch 74/100 - Loss: 3.8623\n",
            "Epoch 75/100 - Loss: 3.8629\n",
            "Epoch 76/100 - Loss: 3.8612\n",
            "Epoch 77/100 - Loss: 3.8630\n",
            "Epoch 78/100 - Loss: 3.8631\n",
            "Epoch 79/100 - Loss: 3.8622\n",
            "Epoch 80/100 - Loss: 3.8625\n",
            "Epoch 81/100 - Loss: 3.8623\n",
            "Epoch 82/100 - Loss: 3.8611\n",
            "Epoch 83/100 - Loss: 3.8631\n",
            "Epoch 84/100 - Loss: 3.8614\n",
            "Epoch 85/100 - Loss: 3.8635\n",
            "Epoch 86/100 - Loss: 3.8618\n",
            "Epoch 87/100 - Loss: 3.8644\n",
            "Epoch 88/100 - Loss: 3.8628\n",
            "Epoch 89/100 - Loss: 3.8613\n",
            "Epoch 90/100 - Loss: 3.8604\n",
            "Epoch 91/100 - Loss: 3.8604\n",
            "Epoch 92/100 - Loss: 3.8616\n",
            "Epoch 93/100 - Loss: 3.8610\n",
            "Epoch 94/100 - Loss: 3.8600\n",
            "Epoch 95/100 - Loss: 3.8618\n",
            "Epoch 96/100 - Loss: 3.8577\n",
            "Epoch 97/100 - Loss: 3.8620\n",
            "Epoch 98/100 - Loss: 3.8615\n",
            "Epoch 99/100 - Loss: 3.8635\n",
            "Epoch 100/100 - Loss: 3.8631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_proj.eval()\n",
        "text_proj.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_proj_all = F.normalize(image_proj(img_tensor_pooled), dim=1)  # [M, 256]\n",
        "    txt_proj_all = F.normalize(text_proj(text_tensor), dim=1)         # [7, 256]\n",
        "\n",
        "    similarity = torch.matmul(img_proj_all, txt_proj_all.T)           # [M, 7]\n",
        "    probs = torch.sigmoid(similarity).cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "03Inpo2RifDy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, hamming_loss\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_thresh = 0.5\n",
        "\n",
        "for t in np.arange(0.1, 0.9, 0.01):\n",
        "    preds_t = (probs > t).astype(int)\n",
        "    f1 = f1_score(labels_np, preds_t, average='macro', zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(f\"‚úÖ Best Threshold: {best_thresh:.2f} ‚Üí F1: {best_f1:.4f}\")\n",
        "\n",
        "y_pred = (probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nüîç Classification Report:\")\n",
        "print(classification_report(labels_np, y_pred, target_names=disease_columns, zero_division=0))\n",
        "\n",
        "hl = hamming_loss(labels_np, y_pred)\n",
        "print(f\"üîª Hamming Loss: {hl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnqw1tg-iiRC",
        "outputId": "5c90a40c-1e7e-4260-e02c-5d2be5fd79e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Best Threshold: 0.57 ‚Üí F1: 0.3516\n",
            "\n",
            "üîç Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "     Atelectasis       0.12      0.33      0.18      9934\n",
            "    Cardiomegaly       0.33      0.71      0.45      8062\n",
            "   Consolidation       0.12      0.44      0.19      4432\n",
            "           Edema       0.49      0.77      0.60     15602\n",
            "Pleural Effusion       0.74      0.73      0.73     25776\n",
            "       Pneumonia       0.03      0.50      0.06      1792\n",
            "    Pneumothorax       0.15      0.58      0.24      5797\n",
            "\n",
            "       micro avg       0.29      0.65      0.40     71395\n",
            "       macro avg       0.28      0.58      0.35     71395\n",
            "    weighted avg       0.45      0.65      0.51     71395\n",
            "     samples avg       0.33      0.45      0.35     71395\n",
            "\n",
            "üîª Hamming Loss: 0.2946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yimOFPrvqUpd"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}